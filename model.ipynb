{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test_no_labels.csv')\n",
    "df_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>BERT</h1>\n",
    "<p>BERT (Bidirectional Encoder Representations from Transformers) ist ein vortrainiertes Sprachmodell von Google, das darauf ausgelegt ist, natürliche Sprache zu verstehen. Es verwendet die Transformer-Architektur und ist bidirektional, was bedeutet, dass es den Kontext eines Wortes sowohl aus dem vorhergehenden als auch aus dem nachfolgenden Text berücksichtigt. Es wurde auf großen Textmengen wie Wikipedia vortrainiert und kann durch Feinabstimmung an spezifische Aufgaben angepasst werden. BERT ist besonders gut geeignet für NLP-Aufgaben wie Textklassifikation, Fragebeantwortung oder Übersetzungen.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
